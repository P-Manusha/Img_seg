# -*- coding: utf-8 -*-
"""App.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrkIe0eoHBx78j7BFjFwdkhicbqtaLVv
"""

# app.py
"""
VisionSeg Streamlit App (single-file)

Features implemented to match your requirements:
- Top title and subtitle
- Example block that shows a random example (from ./examples) on each app load (original + mask if present)
- "Upload image" button that toggles two options: From device and From URL (same tab URL load)
- Only a single image allowed per run
- Model loading: supports TorchScript scripted model (.pt) or a checkpoint (.pth/.pt) containing a state_dict or a dict with 'model_state'/'state_dict'
- If no model is loaded, a dummy mask is shown
- Shows Original and Predicted mask side-by-side, plus a masked preview
- Comparison tab (before/after slider) using streamlit-image-comparison if installed; fallback to side-by-side
- Overlay opacity control
- Blur-background option + intensity slider (applies gaussian blur to background, keeps subject sharp)
- Download buttons for mask, cutout, and blurred background image

Make sure to change TRAINED_MODEL_PATH, MODEL_ARCH, and NUM_CLASSES at the top to match your model (or upload model via sidebar at runtime).
"""

import io
import random
from pathlib import Path
from typing import Tuple

import numpy as np
from PIL import Image, ImageFilter
import streamlit as st

# ML libs
import torch
import torch.nn.functional as F
import torchvision.models.segmentation as segmentation_models
import cv2
from scipy import ndimage as ndi

# Optional 3rd-party niceties
try:
    from streamlit_image_comparison import image_comparison
    HAS_IMAGE_COMPARISON = True
except Exception:
    HAS_IMAGE_COMPARISON = False

# Optional CRF
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    CRF_AVAILABLE = True
except Exception:
    CRF_AVAILABLE = False

# ----------------------
# User config (change these)
# ----------------------
TRAINED_MODEL_PATH = "/kaggle/working/best_seg_model.pth" # set to a path like 'models/my_model_scripted.pt' to auto-load on start, or leave None to upload via sidebar
MODEL_ARCH = "deeplabv3_resnet50"  # or 'fcn_resnet50'
NUM_CLASSES = 2
DEFAULT_INPUT_SIZE = (512, 512)  # (H, W)
EXAMPLES_DIR = Path("examples")

# ----------------------
# Helpers
# ----------------------

def set_page():
    st.set_page_config(page_title="VisionSeg", layout="wide")

@st.cache_data
def list_example_pairs():
    if not EXAMPLES_DIR.exists():
        return []
    imgs = sorted([p for p in EXAMPLES_DIR.iterdir() if p.suffix.lower() in ('.jpg', '.jpeg', '.png') and '_mask' not in p.stem])
    pairs = []
    for p in imgs:
        mask = p.with_name(p.stem + '_mask.png')
        pairs.append((p, mask if mask.exists() else None))
    return pairs

def pil_from_bytes(b: bytes) -> Image.Image:
    return Image.open(io.BytesIO(b)).convert('RGB')

# ----------------------
# Model utilities
# ----------------------

def get_model_arch(name: str = 'deeplabv3_resnet50', num_classes: int = 2):
    name = name.lower()
    if name == 'deeplabv3_resnet50':
        return segmentation_models.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=num_classes)
    elif name == 'fcn_resnet50':
        return segmentation_models.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes)
    else:
        raise ValueError('Unsupported model architecture: ' + name)

@st.cache_resource
def load_model_from_bytes(checkpoint_bytes: bytes, arch: str = MODEL_ARCH, num_classes: int = NUM_CLASSES, device: str = 'cpu'):
    device = torch.device(device)
    # Try TorchScript first
    try:
        scripted = torch.jit.load(io.BytesIO(checkpoint_bytes), map_location=device)
        scripted.eval().to(device)
        return scripted
    except Exception:
        pass

    # Otherwise torch.load
    state = torch.load(io.BytesIO(checkpoint_bytes), map_location=device)
    if isinstance(state, torch.nn.Module):
        state.eval().to(device)
        return state

    sd = None
    if isinstance(state, dict):
        if 'model_state' in state:
            sd = state['model_state']
        elif 'state_dict' in state:
            sd = state['state_dict']
        elif 'model' in state and isinstance(state['model'], dict):
            sd = state['model']
        else:
            sd = state

    if sd is None:
        raise RuntimeError('Could not interpret checkpoint format. Save a scripted model or a state_dict/dict with model_state.')

    model = get_model_arch(arch, num_classes=num_classes)
    # remove 'module.' prefixes if present
    new_sd = {}
    for k, v in sd.items():
        new_k = k.replace('module.', '') if k.startswith('module.') else k
        new_sd[new_k] = v
    model.load_state_dict(new_sd)
    model.eval().to(device)
    return model

# ----------------------
# Pre/post processing
# ----------------------

def preprocess_image_cv2(img_rgb: np.ndarray, size: Tuple[int, int]):
    h, w = size
    img = cv2.resize(img_rgb, (w, h), interpolation=cv2.INTER_LINEAR).astype(np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
    img = (img - mean[None, None, :]) / std[None, None, :]
    img_chw = img.transpose(2, 0, 1)
    tensor = torch.from_numpy(img_chw).unsqueeze(0).float()
    return tensor

def crf_refine(prob_fg: np.ndarray, img_rgb: np.ndarray, n_iters: int = 5):
    if not CRF_AVAILABLE:
        return prob_fg
    h, w = prob_fg.shape
    probs = np.stack([1.0 - prob_fg, prob_fg], axis=0).astype(np.float32)
    d = dcrf.DenseCRF2D(w, h, 2)
    unary = unary_from_softmax(probs)
    d.setUnaryEnergy(unary)
    d.addPairwiseGaussian(sxy=3, compat=3)
    d.addPairwiseBilateral(sxy=60, srgb=5, rgbim=img_rgb, compat=5)
    Q = d.inference(n_iters)
    res = np.array(Q).reshape((2, h, w))[1]
    return np.clip(res, 0.0, 1.0).astype(np.float32)


def predict_with_tta(img_rgb: np.ndarray, model, device: str, size: Tuple[int, int], tta_flip: bool = True, use_crf: bool = False, crf_iters: int = 5):
    device_t = torch.device(device)
    inp = preprocess_image_cv2(img_rgb, size).to(device_t)
    with torch.no_grad():
        out = model(inp)['out']
        prob = F.softmax(out, dim=1).cpu().numpy()[0, 1, :, :]
    probs = [prob]
    if tta_flip:
        img_f = img_rgb[:, ::-1, :].copy()
        inp_f = preprocess_image_cv2(img_f, size).to(device_t)
        with torch.no_grad():
            out_f = model(inp_f)['out']
            prob_f = F.softmax(out_f, dim=1).cpu().numpy()[0, 1, :, :]
        prob_f = prob_f[:, ::-1]
        probs.append(prob_f)
    avg_prob = np.mean(np.stack(probs, axis=0), axis=0)
    if use_crf and CRF_AVAILABLE:
        resized = cv2.resize(img_rgb, (size[1], size[0]), interpolation=cv2.INTER_LINEAR)
        avg_prob = crf_refine(avg_prob.astype(np.float32), resized, n_iters=crf_iters)
    return avg_prob.astype(np.float32)


def postprocess_mask(prob_map: np.ndarray, fg_thresh: float = 0.5, min_area: int = 300, keep_largest: bool = False):
    mask = (prob_map >= fg_thresh).astype(np.uint8)
    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))
    kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_close)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_open)
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)
    out = np.zeros_like(mask)
    comps = []
    for lab in range(1, num_labels):
        area = stats[lab, cv2.CC_STAT_AREA]
        if area >= min_area:
            comps.append((lab, area))
            out[labels == lab] = 1
    if keep_largest and len(comps) > 0:
        comps.sort(key=lambda x: x[1], reverse=True)
        largest = comps[0][0]
        out = (labels == largest).astype(np.uint8)
    out = ndi.binary_fill_holes(out).astype(np.uint8)
    out = cv2.morphologyEx(out.astype(np.uint8), cv2.MORPH_OPEN, kernel_open)
    return (out * 255).astype(np.uint8)

# ----------------------
# Blur background utility
# ----------------------

def blur_background(img_pil: Image.Image, mask_pil: Image.Image, blur_radius: int = 15) -> Image.Image:
    # img_pil: RGB PIL.Image, mask_pil: L-mode (0 or 255)
    img_np = np.array(img_pil)
    mask_np = np.array(mask_pil.convert('L'))
    fg = (mask_np > 127).astype(np.uint8)
    # Apply blur to whole image using OpenCV
    blurred = cv2.GaussianBlur(img_np, (0, 0), sigmaX=blur_radius)
    # Composite: keep foreground from original, background from blurred
    fg3 = fg[:, :, None]
    out = img_np.copy()
    out = (img_np * fg3 + blurred * (1 - fg3)).astype(np.uint8)
    return Image.fromarray(out)

# ----------------------
# Dummy mask generator (when no model loaded)
# ----------------------

def make_dummy_mask(img: Image.Image) -> Image.Image:
    w, h = img.size
    mask = Image.new('L', (w, h), 0)
    draw = Image.fromarray(np.zeros((h, w), dtype=np.uint8))
    # draw ellipse using PIL ImageDraw
    from PIL import ImageDraw
    d = ImageDraw.Draw(mask)
    d.ellipse((w * 0.1, h * 0.1, w * 0.9, h * 0.9), fill=255)
    return mask

# ----------------------
# App UI
# ----------------------

def app():
    set_page()
    st.title('üëÅÔ∏è VisionSeg')
    st.markdown("<div style='color:gray'>Upload one image ‚Äî see subject extraction and optionally blur background</div>", unsafe_allow_html=True)

    # Example block
    st.markdown('---')
    st.header('Example')
    ex_pairs = list_example_pairs()
    if ex_pairs:
        orig_path, mask_path = random.choice(ex_pairs)
        c1, c2 = st.columns(2)
        with c1:
            st.image(str(orig_path), caption='Example ‚Äî Original', use_column_width=True)
        with c2:
            if mask_path:
                st.image(str(mask_path), caption='Example ‚Äî Mask', use_column_width=True)
            else:
                img = Image.open(orig_path).convert('RGB')
                mask = make_dummy_mask(img)
                st.image(composite_with_mask(img, mask), caption='Example ‚Äî Mask Preview', use_column_width=True)
    else:
        st.info('No example images found in ./examples. Add images (and optional *_mask.png) to see rotating examples here.')

    st.markdown('---')
    st.subheader('Upload an image')
    st.write('Click "Upload image" to reveal options. Only one image allowed per run.')

    # upload toggle
    if 'show_upload_options' not in st.session_state:
        st.session_state.show_upload_options = False
    if st.button('Upload image'):
        st.session_state.show_upload_options = not st.session_state.show_upload_options

    uploaded_file = None
    if st.session_state.show_upload_options:
        colA, colB = st.columns(2)
        with colA:
            st.markdown('**From device**')
            uploaded_file = st.file_uploader('Choose one image', type=['jpg', 'jpeg', 'png', 'bmp', 'webp'], accept_multiple_files=False, key='device_uploader')
        with colB:
            st.markdown('**From URL**')
            url = st.text_input('Image URL', key='img_url')
            if st.button('Load from URL'):
                if url:
                    try:
                        import requests
                        r = requests.get(url, timeout=10)
                        r.raise_for_status()
                        uploaded_file = io.BytesIO(r.content)
                        uploaded_file.name = Path(url).name or 'web.jpg'
                        st.success('Loaded image from URL')
                    except Exception as e:
                        st.error(f'Failed to load URL: {e}')
                else:
                    st.error('Please paste an image URL')

    st.markdown('---')

    # Sidebar controls for model and inference
    st.sidebar.header('Model & Inference')
    model_upload = None
    if TRAINED_MODEL_PATH is None:
        model_upload = st.sidebar.file_uploader('Upload checkpoint (.pth/.pt)', type=['pth', 'pt'], accept_multiple_files=False)
    else:
        st.sidebar.write(f'Auto-load model: {TRAINED_MODEL_PATH}')

    device = st.sidebar.selectbox('Device', options=['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu'])
    model_arch = st.sidebar.selectbox('Model arch', options=['deeplabv3_resnet50', 'fcn_resnet50'], index=0)
    num_classes = st.sidebar.number_input('Num classes', min_value=1, max_value=50, value=NUM_CLASSES)
    input_h = st.sidebar.number_input('Input height', min_value=64, max_value=2048, value=DEFAULT_INPUT_SIZE[0])
    input_w = st.sidebar.number_input('Input width', min_value=64, max_value=2048, value=DEFAULT_INPUT_SIZE[1])

    st.sidebar.markdown('---')
    tta = st.sidebar.checkbox('TTA: horizontal flip', value=True)
    use_crf = st.sidebar.checkbox(f'CRF refine (installed={CRF_AVAILABLE})', value=False, disabled=not CRF_AVAILABLE)
    crf_iters = st.sidebar.slider('CRF iterations', 1, 20, 5)
    thresh = st.sidebar.slider('Mask threshold', 0.0, 1.0, 0.5, 0.01)
    min_area = st.sidebar.number_input('Min area (px)', min_value=1, max_value=100000, value=300)
    keep_largest = st.sidebar.checkbox('Keep only largest component', value=False)

    st.sidebar.markdown('---')
    st.sidebar.write('Output options')
    blur_enabled = st.sidebar.checkbox('Blur background (apply blur to background)', value=False)
    blur_radius = st.sidebar.slider('Blur intensity (sigma)', 0, 50, 15)
    overlay_opacity = st.sidebar.slider('Overlay opacity (%)', 0, 100, 60)

    # Load model bytes (auto-load path or uploaded)
    model = None
    model_bytes = None
    if TRAINED_MODEL_PATH is not None and Path(TRAINED_MODEL_PATH).exists():
        try:
            model_bytes = Path(TRAINED_MODEL_PATH).read_bytes()
        except Exception as e:
            st.sidebar.error(f'Failed to read TRAINED_MODEL_PATH: {e}')
    elif model_upload is not None:
        try:
            model_bytes = model_upload.getvalue()
        except Exception as e:
            st.sidebar.error(f'Failed to read uploaded file: {e}')

    if model_bytes is not None:
        try:
            with st.spinner('Loading model...'):
                model = load_model_from_bytes(model_bytes, arch=model_arch, num_classes=int(num_classes), device=device)
            st.sidebar.success('Model loaded')
        except Exception as e:
            st.sidebar.error(f'Model load failed: {e}')
            model = None

    # Process uploaded image (single only)
    if uploaded_file is not None:
        try:
            img = Image.open(uploaded_file).convert('RGB')
        except Exception as e:
            st.error(f'Unable to open uploaded image: {e}')
            return

        st.header('Result')
        left_col, right_col = st.columns([2, 1])
        with left_col:
            # Tabs: Original | Mask | Cutout | Comparison
            tabs = st.tabs(['Original', 'Mask', 'Cutout', 'Comparison'])

            with tabs[0]:
                st.image(img, caption='Original', use_column_width=True)

            # We'll compute mask below; display placeholder while running

        # Compute mask (in right column area display options and run button)
        with right_col:
            st.markdown('**Inference Controls**')
            run_btn = st.button('Run')
            st.markdown('---')
            st.write('Adjust threshold / TTA / CRF / Blur before running')

        if run_btn:
            with st.spinner('Running model...'):
                if model is None:
                    st.warning('No model loaded ‚Äî using dummy mask')
                    mask_pil = make_dummy_mask(img)
                else:
                    img_np = np.array(img)
                    prob = predict_with_tta(img_np, model, device=device, size=(int(input_h), int(input_w)), tta_flip=tta, use_crf=use_crf, crf_iters=int(crf_iters))
                    pred_mask = postprocess_mask(prob, fg_thresh=float(thresh), min_area=int(min_area), keep_largest=bool(keep_largest))
                    mask_pil = Image.fromarray(pred_mask).resize(img.size, resample=Image.NEAREST)

                # masked preview (cutout)
                cutout = Image.new('RGBA', img.size)
                cutout = composite_with_mask(img, mask_pil)

                # blurred background if requested
                if blur_enabled:
                    blurred_img = blur_background(img, mask_pil, blur_radius)
                else:
                    blurred_img = None

                # Show results in the left tabs
                with tabs[1]:
                    st.image(mask_pil, caption='Predicted mask', use_column_width=True)
                with tabs[2]:
                    st.image(cutout, caption='Masked preview (cutout)', use_column_width=True)
                with tabs[3]:
                    # Comparison: use library if available
                    if HAS_IMAGE_COMPARISON:
                        # the component accepts filepaths or PILs; pass PILs
                        image_comparison(img1=img, img2=cutout, label1='Original', label2='Masked')
                    else:
                        c1, c2 = st.columns(2)
                        with c1:
                            st.image(img, caption='Original', use_column_width=True)
                        with c2:
                            st.image(cutout, caption='Masked preview', use_column_width=True)

                # Overlay opacity preview area below tabs
                st.markdown('---')
                st.write('Overlay preview (opacity controlled in sidebar)')
                # build overlay composite with alpha from mask and overlay_opacity
                alpha_val = int(overlay_opacity * 255 / 100)
                overlay = Image.new('RGBA', img.size, (255, 0, 0, alpha_val))
                overlay_masked = Image.composite(overlay, Image.new('RGBA', img.size, (0, 0, 0, 0)), mask_pil.convert('L'))
                combined = Image.alpha_composite(img.convert('RGBA'), overlay_masked).convert('RGB')
                st.image(combined, caption='Overlay preview', use_column_width=True)

                # Download buttons
                buf_mask = io.BytesIO(); mask_pil.save(buf_mask, format='PNG'); buf_mask.seek(0)
                buf_cut = io.BytesIO(); cutout.save(buf_cut, format='PNG'); buf_cut.seek(0)
                st.download_button('Download mask (PNG)', data=buf_mask, file_name='mask.png', mime='image/png')

                buf_cut_flat = io.BytesIO(); cutout.convert('RGB').save(buf_cut_flat, format='PNG'); buf_cut_flat.seek(0)
                st.download_button('Download cutout (PNG)', data=buf_cut_flat, file_name='cutout.png', mime='image/png')

                if blurred_img is not None:
                    buf_blur = io.BytesIO(); blurred_img.save(buf_blur, format='PNG'); buf_blur.seek(0)
                    st.download_button('Download background-blurred image', data=buf_blur, file_name='blurred.png', mime='image/png')

        else:
            st.info('Click Run to perform inference and produce mask/cutout.')

    else:
        st.info('No image selected yet ‚Äî click "Upload image" and choose an option.')

    st.markdown('---')
    st.markdown('**Thanks**')
    st.write('\n'.join([
        '‚ÄúThanks for trying this demo ‚Äî happy experimenting!‚Äù',
        '‚ÄúUse the blur option to stylize background while keeping the subject sharp.‚Äù'
    ]))

if __name__ == '__main__':
    app()